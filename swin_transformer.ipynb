{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore.common.initializer as weight_init\n",
    "import mindspore.ops.operations as P\n",
    "from mindspore import Parameter\n",
    "from mindspore import Tensor\n",
    "from mindspore import dtype as mstype\n",
    "from mindspore import nn\n",
    "from mindspore import numpy\n",
    "from mindspore import ops\n",
    "\n",
    "import os\n",
    "import mindspore.common.dtype as mstype\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.transforms as C\n",
    "import mindspore.dataset.vision as vision\n",
    "from mindspore.dataset.vision.utils import Inter\n",
    "\n",
    "import collections.abc\n",
    "from itertools import repeat\n",
    "\n",
    "import os\n",
    "\n",
    "from mindspore import Model\n",
    "from mindspore import context\n",
    "from mindspore.common import set_seed\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor, Callback\n",
    "from mindspore.nn.loss.loss import LossBase\n",
    "from mindspore.common import RowTensor\n",
    "from mindspore.ops import composite as Cps\n",
    "from mindspore.ops import functional as F\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore.communication.management import init, get_rank\n",
    "from mindspore.context import ParallelMode\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.nn.optim import AdamWeightDecay\n",
    "from mindspore.nn.optim.momentum import Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define SwinTransformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from src.models.swintransformer.misc import _ntuple, Identity, DropPath1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Reading YAML config from ./src/configs/swin_tiny_patch4_window7_224.yaml\n",
      "Namespace(arch='swin_tiny_patch4_window7_224', accumulation_step=1, amp_level='O1', ape=False, batch_size=128, beta=[0.9, 0.999], clip_global_norm_value=5.0, crop=True, data_url='./src/data/imagenet', device_id=1, device_num=1, device_target='GPU', epochs=5, eps=1e-08, file_format='MINDIR', in_channel=3, is_dynamic_loss_scale=True, keep_checkpoint_max=20, optimizer='adamw', set='ImageNet', graph_mode=0, mix_up=0.8, mlp_ratio=4.0, num_parallel_workers=16, start_epoch=0, warmup_length=20, warmup_lr=7e-08, weight_decay=0.05, loss_scale=1024, lr=0.0005, lr_scheduler='cosine_lr', lr_adjust=30, lr_gamma=0.97, momentum=0.9, num_classes=1000, patch_size=4, patch_norm=True, swin_config='./src/configs/swin_tiny_patch4_window7_224.yaml', seed=0, save_every=2, label_smoothing=0.1, image_size=224, train_url='./', cutmix=1.0, auto_augment='rand-m9-mstd0.5-inc1', interpolation='bicubic', re_prob=0.25, re_mode='pixel', re_count=1, mixup_prob=1.0, switch_prob=0.5, mixup_mode='batch', base_lr=0.0005, min_lr=6e-06, nonlinearity='GELU', keep_bn_fp32=True, drop_path_rate=0.2, embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24], window_size=7)\n"
     ]
    }
   ],
   "source": [
    "from src.configs.args import args\n",
    "\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, collections.abc.Iterable):\n",
    "            return x\n",
    "        return tuple(repeat(x, n))\n",
    "\n",
    "    return parse\n",
    "\n",
    "class Identity(nn.Cell):\n",
    "    \"\"\"Identity\"\"\"\n",
    "    def construct(self, x):\n",
    "        return x\n",
    "\n",
    "class DropPath(nn.Cell):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob, ndim):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop = nn.Dropout(keep_prob=1 - drop_prob)\n",
    "        shape = (1,) + (1,) * (ndim + 1)\n",
    "        self.ndim = ndim\n",
    "        self.mask = Tensor(np.ones(shape), dtype=mstype.float32)\n",
    "\n",
    "    def construct(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "        mask = ops.Tile()(self.mask, (x.shape[0],) + (1,) * (self.ndim + 1))\n",
    "        out = self.drop(mask)\n",
    "        out = out * x\n",
    "        return out\n",
    "\n",
    "\n",
    "class DropPath1D(DropPath):\n",
    "    def __init__(self, drop_prob):\n",
    "        super(DropPath1D, self).__init__(drop_prob=drop_prob, ndim=1)\n",
    "\n",
    "to_2tuple = _ntuple(2)\n",
    "\n",
    "act_layers = {\n",
    "    \"GELU\": nn.GELU,\n",
    "    \"gelu\": nn.GELU,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Cell):\n",
    "    \"\"\"MLP Cell\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None,\n",
    "                 out_features=None,\n",
    "                 act_layer=act_layers[args.nonlinearity],\n",
    "                 drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Dense(in_channels=in_features, out_channels=hidden_features, has_bias=True)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Dense(in_channels=hidden_features, out_channels=out_features, has_bias=True)\n",
    "        self.drop = nn.Dropout(keep_prob=1.0 - drop)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = np.reshape(x, (B, H // window_size, window_size, W // window_size, window_size, C))\n",
    "    windows = x.transpose(0, 1, 3, 2, 4, 5).reshape(-1, window_size, window_size, C)\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowPartitionConstruct(nn.Cell):\n",
    "    \"\"\"WindowPartitionConstruct Cell\"\"\"\n",
    "\n",
    "    def __init__(self, window_size):\n",
    "        super(WindowPartitionConstruct, self).__init__()\n",
    "\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, H, W, C)\n",
    "            window_size (int): window size\n",
    "\n",
    "        Returns:\n",
    "            windows: (num_windows*B, window_size, window_size, C)\n",
    "        \"\"\"\n",
    "        B, H, W, C = x.shape\n",
    "        x = P.Reshape()(x, (B, H // self.window_size, self.window_size, W // self.window_size, self.window_size, C))\n",
    "        x = P.Transpose()(x, (0, 1, 3, 2, 4, 5))\n",
    "        x = P.Reshape()(x, (B * H * W // (self.window_size ** 2), self.window_size, self.window_size, C))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowReverseConstruct(nn.Cell):\n",
    "    \"\"\"WindowReverseConstruct Cell\"\"\"\n",
    "\n",
    "    def construct(self, windows, window_size, H, W):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            windows: (num_windows*B, window_size, window_size, C)\n",
    "            window_size (int): Window size\n",
    "            H (int): Height of image\n",
    "            W (int): Width of image\n",
    "\n",
    "        Returns:\n",
    "            x: (B, H, W, C)\n",
    "        \"\"\"\n",
    "        B = windows.shape[0] // (H * W // window_size // window_size)\n",
    "        x = ops.Reshape()(windows, (B, H // window_size, W // window_size, window_size, window_size, -1))\n",
    "        x = ops.Transpose()(x, (0, 1, 3, 2, 4, 5))\n",
    "        x = ops.Reshape()(x, (B, H, W, -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeBias(nn.Cell):\n",
    "    \"\"\"RelativeBias Cell\"\"\"\n",
    "\n",
    "    def __init__(self, window_size, num_heads):\n",
    "        super(RelativeBias, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        # define a parameter table of relative position bias\n",
    "        coords_h = np.arange(self.window_size[0]).reshape(self.window_size[0], 1).repeat(self.window_size[0],\n",
    "                                                                                         1).reshape(1, -1)\n",
    "        coords_w = np.arange(self.window_size[1]).reshape(1, self.window_size[1]).repeat(self.window_size[1],\n",
    "                                                                                         0).reshape(1, -1)\n",
    "        coords_flatten = np.concatenate([coords_h, coords_w], axis=0)  # 2, Wh, Ww\n",
    "        relative_coords = coords_flatten[:, :, np.newaxis] - coords_flatten[:, np.newaxis, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.transpose(1, 2, 0)  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        self.relative_position_index = Tensor(relative_coords.sum(-1).reshape(-1))  # Wh*Ww, Wh*Ww\n",
    "        self.relative_position_bias_table = Parameter(\n",
    "            Tensor(np.random.randn((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads),\n",
    "                   dtype=mstype.float32))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "        self.one_hot = nn.OneHot(axis=-1, depth=(2 * window_size[0] - 1) * (2 * window_size[1] - 1),\n",
    "                                 dtype=mstype.float32)\n",
    "        self.index = Parameter(self.one_hot(self.relative_position_index), requires_grad=False)\n",
    "\n",
    "    def construct(self, axis=0):\n",
    "        out = ops.MatMul()(self.index, self.relative_position_bias_table)\n",
    "        out = P.Reshape()(out, (self.window_size[0] * self.window_size[1],\n",
    "                                self.window_size[0] * self.window_size[1], -1))\n",
    "        out = P.Transpose()(out, (2, 0, 1))\n",
    "        out = ops.ExpandDims()(out, 0)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Cell):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) Cell with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qZk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        if isinstance(dim, tuple) and len(dim) == 1:\n",
    "            dim = dim[0]\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = Tensor(qk_scale or head_dim ** -0.5, mstype.float32)\n",
    "        self.relative_bias = RelativeBias(self.window_size, num_heads)\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        self.q = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n",
    "        self.k = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n",
    "        self.v = nn.Dense(in_channels=dim, out_channels=dim, has_bias=qkv_bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(keep_prob=1.0 - attn_drop)\n",
    "        self.proj = nn.Dense(in_channels=dim, out_channels=dim, has_bias=True)\n",
    "        self.proj_drop = nn.Dropout(keep_prob=1.0 - proj_drop)\n",
    "        self.softmax = nn.Softmax(axis=-1)\n",
    "\n",
    "    def construct(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        q = ops.Reshape()(self.q(x), (B_, N, self.num_heads, C // self.num_heads)) * self.scale\n",
    "        q = ops.Transpose()(q, (0, 2, 1, 3))\n",
    "        k = ops.Reshape()(self.k(x), (B_, N, self.num_heads, C // self.num_heads))\n",
    "        k = ops.Transpose()(k, (0, 2, 3, 1))\n",
    "        v = ops.Reshape()(self.v(x), (B_, N, self.num_heads, C // self.num_heads))\n",
    "        v = ops.Transpose()(v, (0, 2, 1, 3))\n",
    "\n",
    "        attn = ops.BatchMatMul()(q, k)\n",
    "        attn = attn + self.relative_bias()\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[1]\n",
    "            attn = P.Reshape()(attn, (B_ // nW, nW, self.num_heads, N, N,)) + mask\n",
    "            attn = P.Reshape()(attn, (-1, self.num_heads, N, N,))\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = ops.Reshape()(ops.Transpose()(ops.BatchMatMul()(attn, v), (0, 2, 1, 3)), (B_, N, C))\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Roll(nn.Cell):\n",
    "    \"\"\"Roll Cell\"\"\"\n",
    "\n",
    "    def __init__(self, shift_size, shift_axis=(1, 2)):\n",
    "        super(Roll, self).__init__()\n",
    "        self.shift_size = to_2tuple(shift_size)\n",
    "        self.shift_axis = shift_axis\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = numpy.roll(x, self.shift_size, self.shift_axis)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Cell):\n",
    "    \"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Cell, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Cell, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=act_layers[args.nonlinearity], norm_layer=nn.LayerNorm):\n",
    "        super(SwinTransformerBlock, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "\n",
    "        if isinstance(dim, int):\n",
    "            dim = (dim,)\n",
    "\n",
    "        self.norm1 = norm_layer(dim, epsilon=1e-5)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath1D(drop_path) if drop_path > 0. else Identity()\n",
    "        self.norm2 = norm_layer(dim, epsilon=1e-5)\n",
    "        mlp_hidden_dim = int((dim[0] if isinstance(dim, tuple) else dim) * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim[0] if isinstance(dim, tuple) else dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer, drop=drop)\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = np.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "            # img_mask: [1, 56, 56, 1] window_size: 7\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.reshape(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows[:, np.newaxis] - mask_windows[:, :, np.newaxis]\n",
    "            # [64, 49, 49] ==> [1, 64, 1, 49, 49]\n",
    "            attn_mask = np.expand_dims(attn_mask, axis=1)\n",
    "            attn_mask = np.expand_dims(attn_mask, axis=0)\n",
    "            attn_mask = Tensor(np.where(attn_mask == 0, 0., -100.), dtype=mstype.float32)\n",
    "            self.attn_mask = Parameter(attn_mask, requires_grad=False)\n",
    "            self.roll_pos = Roll(self.shift_size)\n",
    "            self.roll_neg = Roll(-self.shift_size)\n",
    "        else:\n",
    "            self.attn_mask = None\n",
    "\n",
    "        self.window_partition = WindowPartitionConstruct(self.window_size)\n",
    "        self.window_reverse = WindowReverseConstruct()\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"construct function\"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, _, C = x.shape\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = P.Reshape()(x, (B, H, W, C,))\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = self.roll_neg(x)\n",
    "            # shifted_x = numpy.roll(x, (-self.shift_size, -self.shift_size), (1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = self.window_partition(shifted_x)  # nW*B, window_size, window_size, C\n",
    "        x_windows = ops.Reshape()(x_windows,\n",
    "                                  (-1, self.window_size * self.window_size, C,))  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = P.Reshape()(attn_windows, (-1, self.window_size, self.window_size, C,))\n",
    "        shifted_x = self.window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = self.roll_pos(shifted_x)\n",
    "            # x = numpy.roll(shifted_x, (self.shift_size, self.shift_size), (1, 2))  # TODO:Don't stupid\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        x = P.Reshape()(x, (B, H * W, C,))\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Cell):\n",
    "    \"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim[0] if isinstance(dim, tuple) and len(dim) == 1 else dim\n",
    "        # Default False\n",
    "        self.reduction = nn.Dense(in_channels=4 * dim, out_channels=2 * dim, has_bias=False)\n",
    "        self.norm = norm_layer([dim * 4,])\n",
    "        self.H, self.W = self.input_resolution\n",
    "        self.H_2, self.W_2 = self.H // 2, self.W // 2\n",
    "        self.H2W2 = int(self.H * self.W // 4)\n",
    "        self.dim_mul_4 = int(dim * 4)\n",
    "        self.H2W2 = int(self.H * self.W // 4)\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        x = P.Reshape()(x, (B, self.H_2, 2, self.W_2, 2, self.dim))\n",
    "        x = P.Transpose()(x, (0, 1, 3, 4, 2, 5))\n",
    "        x = P.Reshape()(x, (B, self.H2W2, self.dim_mul_4))\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLayer(nn.Cell):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Cell, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Cell | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.CellList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,  # TODO: 这里window_size//2的时候特别慢\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"construct\"\"\"\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Cell):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        image_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Cell, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        image_size = to_2tuple(image_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [image_size[0] // patch_size[0], image_size[1] // patch_size[1]]\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_channels=in_chans, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size,\n",
    "                              pad_mode='pad', has_bias=True, weight_init=\"TruncatedNormal\")\n",
    "\n",
    "        if norm_layer is not None:\n",
    "            if isinstance(embed_dim, int):\n",
    "                embed_dim = (embed_dim,)\n",
    "            self.norm = norm_layer(embed_dim, epsilon=1e-5)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def construct(self, x):\n",
    "        \"\"\"docstring\"\"\"\n",
    "        B = x.shape[0]\n",
    "        # FIXME look at relaxing size constraints\n",
    "        x = ops.Reshape()(self.proj(x), (B, self.embed_dim, -1))  # B Ph*Pw C\n",
    "        x = ops.Transpose()(x, (0, 2, 1))\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Cell):\n",
    "    \"\"\" Swin Transformer\n",
    "        A Pynp impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "\n",
    "    Args:\n",
    "        image_size (int | tuple(int)): Input image size. Default 224\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Cell): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size=224, patch_size=4, in_chans=3, num_classes=1000,\n",
    "                 embed_dim=96, depths=None, num_heads=None, window_size=7,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            image_size=image_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = Parameter(Tensor(np.zeros(1, num_patches, embed_dim), dtype=mstype.float32))\n",
    "\n",
    "        self.pos_drop = nn.Dropout(keep_prob=1.0 - drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x for x in np.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.CellList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer([self.num_features,], epsilon=1e-5)\n",
    "        self.avgpool = P.ReduceMean(keep_dims=False)\n",
    "        self.head = nn.Dense(in_channels=self.num_features,\n",
    "                             out_channels=num_classes, has_bias=True) if num_classes > 0 else Identity()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"init_weights\"\"\"\n",
    "        for _, cell in self.cells_and_names():\n",
    "            if isinstance(cell, nn.Dense):\n",
    "                cell.weight.set_data(weight_init.initializer(weight_init.TruncatedNormal(sigma=0.02),\n",
    "                                                             cell.weight.shape,\n",
    "                                                             cell.weight.dtype))\n",
    "                if isinstance(cell, nn.Dense) and cell.bias is not None:\n",
    "                    cell.bias.set_data(weight_init.initializer(weight_init.Zero(),\n",
    "                                                               cell.bias.shape,\n",
    "                                                               cell.bias.dtype))\n",
    "            elif isinstance(cell, nn.LayerNorm):\n",
    "                cell.gamma.set_data(weight_init.initializer(weight_init.One(),\n",
    "                                                            cell.gamma.shape,\n",
    "                                                            cell.gamma.dtype))\n",
    "                cell.beta.set_data(weight_init.initializer(weight_init.Zero(),\n",
    "                                                           cell.beta.shape,\n",
    "                                                           cell.beta.dtype))\n",
    "\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)  # B L C\n",
    "        x = self.avgpool(ops.Transpose()(x, (0, 2, 1)), 2)  # B C 1\n",
    "        return x\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset - ImageNet Mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "src/data/imagenet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data operations, will be used in train.py and eval.py\n",
    "\"\"\"\n",
    "\n",
    "from src.data.augment.auto_augment import _pil_interp, rand_augment_transform\n",
    "from src.data.augment.mixup import Mixup\n",
    "from src.data.augment.random_erasing import RandomErasing\n",
    "from src.data.data_utils.moxing_adapter import sync_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_rank_info():\n",
    "    \"\"\"\n",
    "    get rank size and rank id\n",
    "    \"\"\"\n",
    "    rank_size = int(os.environ.get(\"RANK_SIZE\", 1))\n",
    "\n",
    "    if rank_size > 1:\n",
    "        from mindspore.communication.management import get_rank, get_group_size\n",
    "        rank_size = get_group_size()\n",
    "        rank_id = get_rank()\n",
    "    else:\n",
    "        rank_size = rank_id = None\n",
    "\n",
    "    return rank_size, rank_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_imagenet(dataset_dir, args, repeat_num=1, training=True):\n",
    "    \"\"\"\n",
    "    create a train or eval imagenet2012 dataset for SwinTransformer\n",
    "\n",
    "    Args:\n",
    "        dataset_dir(string): the path of dataset.\n",
    "        do_train(bool): whether dataset is used for train or eval.\n",
    "        repeat_num(int): the repeat times of dataset. Default: 1\n",
    "\n",
    "    Returns:\n",
    "        dataset\n",
    "    \"\"\"\n",
    "\n",
    "    device_num, rank_id = _get_rank_info()\n",
    "    shuffle = bool(training)\n",
    "    if device_num == 1 or not training:\n",
    "        data_set = ds.ImageFolderDataset(dataset_dir, num_parallel_workers=args.num_parallel_workers,\n",
    "                                         shuffle=shuffle)\n",
    "    else:\n",
    "        data_set = ds.ImageFolderDataset(dataset_dir, num_parallel_workers=args.num_parallel_workers, shuffle=shuffle,\n",
    "                                         num_shards=device_num, shard_id=rank_id)\n",
    "\n",
    "    image_size = args.image_size\n",
    "\n",
    "    # define map operations\n",
    "    # BICUBIC: 3\n",
    "    \n",
    "    # ImageNet一般都要数据增强\n",
    "    if training:\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        aa_params = dict(\n",
    "            translate_const=int(image_size * 0.45),\n",
    "            img_mean=tuple([min(255, round(255 * x)) for x in mean]),\n",
    "        )\n",
    "        interpolation = args.interpolation\n",
    "        auto_augment = args.auto_augment\n",
    "        assert auto_augment.startswith('rand')\n",
    "        aa_params['interpolation'] = _pil_interp(interpolation)\n",
    "\n",
    "        transform_img = [\n",
    "            vision.RandomCropDecodeResize(image_size, scale=(0.08, 1.0), ratio=(3 / 4, 4 / 3),\n",
    "                                          interpolation=Inter.BICUBIC),\n",
    "            vision.RandomHorizontalFlip(prob=0.5),\n",
    "            vision.ToPIL()\n",
    "        ]\n",
    "        transform_img += [rand_augment_transform(auto_augment, aa_params)]\n",
    "        transform_img += [\n",
    "            vision.ToTensor(),\n",
    "            vision.Normalize(mean=mean, std=std, is_hwc=False),\n",
    "            RandomErasing(args.re_prob, mode=args.re_mode, max_count=args.re_count)\n",
    "        ]\n",
    "    else:\n",
    "        mean = [0.485 * 255, 0.456 * 255, 0.406 * 255]\n",
    "        std = [0.229 * 255, 0.224 * 255, 0.225 * 255]\n",
    "        # test transform complete\n",
    "        if args.crop:\n",
    "            transform_img = [\n",
    "                vision.Decode(),\n",
    "                vision.Resize(int(256 / 224 * image_size), interpolation=Inter.BICUBIC),\n",
    "                vision.CenterCrop(image_size),\n",
    "                vision.Normalize(mean=mean, std=std, is_hwc=True),\n",
    "                vision.HWC2CHW()\n",
    "            ]\n",
    "        else:\n",
    "            transform_img = [\n",
    "                vision.Decode(),\n",
    "                vision.Resize(int(image_size), interpolation=Inter.BICUBIC),\n",
    "                vision.Normalize(mean=mean, std=std, is_hwc=True),\n",
    "                vision.HWC2CHW()\n",
    "            ]\n",
    "\n",
    "    transform_label = C.TypeCast(mstype.int32)\n",
    "\n",
    "    data_set = data_set.map(input_columns=\"image\", num_parallel_workers=args.num_parallel_workers,\n",
    "                            operations=transform_img)\n",
    "    data_set = data_set.map(input_columns=\"label\", num_parallel_workers=args.num_parallel_workers,\n",
    "                            operations=transform_label)\n",
    "    if (args.mix_up > 0. or args.cutmix > 0.)  and not training:\n",
    "        # if use mixup and not training(False), one hot val data label\n",
    "        one_hot = C.OneHot(num_classes=args.num_classes)\n",
    "        data_set = data_set.map(input_columns=\"label\", num_parallel_workers=args.num_parallel_workers,\n",
    "                                operations=one_hot)\n",
    "    # apply batch operations\n",
    "    data_set = data_set.batch(args.batch_size, drop_remainder=True,\n",
    "                              num_parallel_workers=args.num_parallel_workers)\n",
    "\n",
    "    if (args.mix_up > 0. or args.cutmix > 0.) and training:\n",
    "        mixup_fn = Mixup(\n",
    "            mixup_alpha=args.mix_up, cutmix_alpha=args.cutmix, cutmix_minmax=None,\n",
    "            prob=args.mixup_prob, switch_prob=args.switch_prob, mode=args.mixup_mode,\n",
    "            label_smoothing=args.label_smoothing, num_classes=args.num_classes)\n",
    "\n",
    "        data_set = data_set.map(operations=mixup_fn, input_columns=[\"image\", \"label\"],\n",
    "                                num_parallel_workers=args.num_parallel_workers)\n",
    "\n",
    "    # apply dataset repeat operation\n",
    "    data_set = data_set.repeat(repeat_num)\n",
    "    ds.config.set_prefetch_size(4)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNet:\n",
    "    \"\"\"ImageNet Define\"\"\"\n",
    "\n",
    "    def __init__(self, args, training=True):\n",
    "        train_dir = os.path.join(args.data_url, \"train\")\n",
    "        val_ir = os.path.join(args.data_url, \"val\")\n",
    "        if training:\n",
    "            self.train_dataset = create_dataset_imagenet(train_dir, training=True, args=args)\n",
    "        self.val_dataset = create_dataset_imagenet(val_ir, training=False, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train  \n",
    "train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from src.tools.callback import EvaluateCallBack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateCallBack(Callback):\n",
    "    \"\"\"EvaluateCallBack\"\"\"\n",
    "\n",
    "    def __init__(self, model, eval_dataset, src_url, train_url, save_freq=50):\n",
    "        super(EvaluateCallBack, self).__init__()\n",
    "        self.model = model\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.src_url = src_url\n",
    "        self.train_url = train_url\n",
    "        self.save_freq = save_freq\n",
    "        self.best_acc = 0.\n",
    "\n",
    "    def epoch_end(self, run_context):\n",
    "        \"\"\"\n",
    "            Test when epoch end, save best model with best.ckpt.\n",
    "        \"\"\"\n",
    "        cb_params = run_context.original_args()\n",
    "        cur_epoch_num = cb_params.cur_epoch_num\n",
    "        result = self.model.eval(self.eval_dataset)\n",
    "        if result[\"acc\"] > self.best_acc:\n",
    "            self.best_acc = result[\"acc\"]\n",
    "        print(\"epoch: %s acc: %s, best acc is %s\" %\n",
    "              (cb_params.cur_epoch_num, result[\"acc\"], self.best_acc), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from src.tools.cell import cast_amp  \n",
    "设置精度水平"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_keep_fp32(network, cell_types):\n",
    "    \"\"\"Cast cell to fp32 if cell in cell_types\"\"\"\n",
    "    for _, cell in network.cells_and_names():\n",
    "        if isinstance(cell, cell_types):\n",
    "            cell.to_float(mstype.float32)\n",
    "\n",
    "\n",
    "def cast_amp(net):\n",
    "    \"\"\"cast network amp_level\"\"\"\n",
    "    if args.amp_level == \"O1\":\n",
    "        print(f\"=> using amp_level {args.amp_level}\\n\"\n",
    "              f\"=> change {args.arch} to fp16\")\n",
    "        net.to_float(mstype.float16)\n",
    "        cell_types = (nn.GELU, nn.Softmax, nn.Conv2d, nn.Conv1d, nn.BatchNorm2d, nn.LayerNorm)\n",
    "        print(f\"=> cast {cell_types} to fp32 back\")\n",
    "        do_keep_fp32(net, cell_types)\n",
    "    elif args.amp_level == \"O2\":\n",
    "        print(f\"=> using amp_level {args.amp_level}\\n\"\n",
    "              f\"=> change {args.arch} to fp16\")\n",
    "        net.to_float(mstype.float16)\n",
    "        cell_types = (nn.BatchNorm2d, nn.LayerNorm)\n",
    "        print(f\"=> cast {cell_types} to fp32 back\")\n",
    "        do_keep_fp32(net, cell_types)\n",
    "    elif args.amp_level == \"O3\":\n",
    "        print(f\"=> using amp_level {args.amp_level}\\n\"\n",
    "              f\"=> change {args.arch} to fp16\")\n",
    "        net.to_float(mstype.float16)\n",
    "    else:\n",
    "        print(f\"=> using amp_level {args.amp_level}\")\n",
    "        args.loss_scale = 1.\n",
    "        args.is_dynamic_loss_scale = 0\n",
    "        print(f\"=> When amp_level is O0, using fixed loss_scale with {args.loss_scale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from src.tools.criterion import get_criterion, NetWithLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftTargetCrossEntropy(LossBase):\n",
    "    \"\"\"SoftTargetCrossEntropy for MixUp Augment\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SoftTargetCrossEntropy, self).__init__()\n",
    "        self.mean_ops = P.ReduceMean(keep_dims=False)\n",
    "        self.sum_ops = P.ReduceSum(keep_dims=False)\n",
    "        self.log_softmax = P.LogSoftmax()\n",
    "\n",
    "    def construct(self, logit, label):\n",
    "        logit = P.Cast()(logit, mstype.float32)\n",
    "        label = P.Cast()(label, mstype.float32)\n",
    "        loss = self.sum_ops(-label * self.log_softmax(logit), -1)\n",
    "        return self.mean_ops(loss)\n",
    "\n",
    "\n",
    "class CrossEntropySmooth(LossBase):\n",
    "    \"\"\"CrossEntropy\"\"\"\n",
    "\n",
    "    def __init__(self, sparse=True, reduction='mean', smooth_factor=0., num_classes=1000):\n",
    "        super(CrossEntropySmooth, self).__init__()\n",
    "        self.onehot = P.OneHot()\n",
    "        self.sparse = sparse\n",
    "        self.on_value = Tensor(1.0 - smooth_factor, mstype.float32)\n",
    "        self.off_value = Tensor(1.0 * smooth_factor / (num_classes - 1), mstype.float32)\n",
    "        self.ce = nn.SoftmaxCrossEntropyWithLogits(reduction=reduction)\n",
    "        self.cast = ops.Cast()\n",
    "\n",
    "    def construct(self, logit, label):\n",
    "        if self.sparse:\n",
    "            label = self.onehot(label, F.shape(logit)[1], self.on_value, self.off_value)\n",
    "        loss2 = self.ce(logit, label)\n",
    "        return loss2\n",
    "\n",
    "\n",
    "def get_criterion(args):\n",
    "    \"\"\"Get loss function from args.label_smooth and args.mix_up\"\"\"\n",
    "    assert args.label_smoothing >= 0. and args.label_smoothing <= 1.\n",
    "\n",
    "    if args.mix_up > 0. or args.cutmix > 0.:\n",
    "        print(25 * \"=\" + \"Using MixBatch\" + 25 * \"=\")\n",
    "        # smoothing is handled with mixup label transform\n",
    "        criterion = SoftTargetCrossEntropy()\n",
    "    elif args.label_smoothing > 0.:\n",
    "        print(25 * \"=\" + \"Using label smoothing\" + 25 * \"=\")\n",
    "        criterion = CrossEntropySmooth(sparse=True, reduction=\"mean\",\n",
    "                                       smooth_factor=args.label_smoothing,\n",
    "                                       num_classes=args.num_classes)\n",
    "    else:\n",
    "        print(25 * \"=\" + \"Using Simple CE\" + 25 * \"=\")\n",
    "        criterion = CrossEntropySmooth(sparse=True, reduction=\"mean\", num_classes=args.num_classes)\n",
    "\n",
    "    return criterion\n",
    "\n",
    "\n",
    "class NetWithLoss(nn.Cell):\n",
    "    \"\"\"\n",
    "       NetWithLoss: Only support Network with Classfication\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, criterion):\n",
    "        super(NetWithLoss, self).__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def construct(self, data, label):\n",
    "        predict = self.model(data)\n",
    "        loss = self.criterion(predict, label)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from src.tools.get_misc import get_dataset, set_device, get_model, pretrained, get_train_one_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(args, training=True):\n",
    "    \"\"\"\"Get model according to args.set\"\"\"\n",
    "    print(f\"=> Getting {args.set} dataset\")\n",
    "    # dataset = getattr(data, args.set)(args, training)\n",
    "    # dataset = getattr(ImageNet, args.set)(args, training)\n",
    "    dataset = ImageNet(args, training)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def set_device(args):\n",
    "    \"\"\"Set device and ParallelMode(if device_num > 1)\"\"\"\n",
    "    rank = 0\n",
    "    # set context and device\n",
    "    device_target = args.device_target\n",
    "    device_num = int(os.environ.get(\"DEVICE_NUM\", 1))\n",
    "\n",
    "    if device_target == \"Ascend\":\n",
    "        if device_num > 1:\n",
    "            context.set_context(device_id=int(os.environ[\"DEVICE_ID\"]))\n",
    "            init(backend_name='hccl')\n",
    "            context.reset_auto_parallel_context()\n",
    "            context.set_auto_parallel_context(device_num=device_num, parallel_mode=ParallelMode.DATA_PARALLEL,\n",
    "                                              gradients_mean=True)\n",
    "            # context.set_auto_parallel_context(pipeline_stages=2, full_batch=True)\n",
    "\n",
    "            rank = get_rank()\n",
    "        else:\n",
    "            context.set_context(device_id=args.device_id)\n",
    "    elif device_target == \"GPU\":\n",
    "        if device_num > 1:\n",
    "            init(backend_name='nccl')\n",
    "            context.reset_auto_parallel_context()\n",
    "            context.set_auto_parallel_context(device_num=device_num, parallel_mode=ParallelMode.DATA_PARALLEL,\n",
    "                                              gradients_mean=True)\n",
    "            rank = get_rank()\n",
    "        else:\n",
    "            context.set_context(device_id=args.device_id)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported platform.\")\n",
    "\n",
    "    return rank\n",
    "\n",
    "_grad_scale = Cps.MultitypeFuncGraph(\"grad_scale\")\n",
    "reciprocal = P.Reciprocal()\n",
    "\n",
    "@_grad_scale.register(\"Tensor\", \"Tensor\")\n",
    "def tensor_grad_scale(scale, grad):\n",
    "    return grad * F.cast(reciprocal(scale), F.dtype(grad))\n",
    "\n",
    "@_grad_scale.register(\"Tensor\", \"RowTensor\")\n",
    "def tensor_grad_scale_row_tensor(scale, grad):\n",
    "    return RowTensor(grad.indices,\n",
    "                     grad.values * F.cast(reciprocal(scale), F.dtype(grad.values)),\n",
    "                     grad.dense_shape)\n",
    "\n",
    "class TrainClipGrad(nn.TrainOneStepWithLossScaleCell):\n",
    "    \"\"\"\n",
    "    Encapsulation class of SSD network training.\n",
    "\n",
    "    Append an optimizer to the training network after that the construct\n",
    "    function can be called to create the backward graph.\n",
    "\n",
    "    Args:\n",
    "        network (Cell): The training network. Note that loss function should have been added.\n",
    "        optimizer (Optimizer): Optimizer for updating the weights.\n",
    "        sens (Number): The adjust parameter. Default: 1.0.\n",
    "        use_global_nrom(bool): Whether apply global norm before optimizer. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network, optimizer,\n",
    "                 scale_sense=1.0, use_global_norm=True,\n",
    "                 clip_global_norm_value=1.0):\n",
    "        super(TrainClipGrad, self).__init__(network, optimizer, scale_sense)\n",
    "        self.use_global_norm = use_global_norm\n",
    "        self.clip_global_norm_value = clip_global_norm_value\n",
    "        self.print = P.Print()\n",
    "\n",
    "    def construct(self, *inputs):\n",
    "        \"\"\"construct\"\"\"\n",
    "        weights = self.weights\n",
    "        loss = self.network(*inputs)\n",
    "        scaling_sens = self.scale_sense\n",
    "\n",
    "        status, scaling_sens = self.start_overflow_check(loss, scaling_sens)\n",
    "\n",
    "        scaling_sens_filled = Cps.ones_like(loss) * F.cast(scaling_sens, F.dtype(loss))\n",
    "        grads = self.grad(self.network, weights)(*inputs, scaling_sens_filled)\n",
    "        grads = self.hyper_map(F.partial(_grad_scale, scaling_sens), grads)\n",
    "        # apply grad reducer on grads\n",
    "        grads = self.grad_reducer(grads)\n",
    "        # get the overflow buffer\n",
    "        cond = self.get_overflow_status(status, grads)\n",
    "        overflow = self.process_loss_scale(cond)\n",
    "        # if there is no overflow, do optimize\n",
    "        if not overflow:\n",
    "            if self.use_global_norm:\n",
    "                grads = Cps.clip_by_global_norm(grads, clip_norm=self.clip_global_norm_value)\n",
    "            loss = F.depend(loss, self.optimizer(grads))\n",
    "        else:\n",
    "            self.print(\"=============Over Flow, skipping=============\")\n",
    "        return loss\n",
    "\n",
    "\n",
    "def get_train_one_step(args, net_with_loss, optimizer):\n",
    "    \"\"\"get_train_one_step cell\"\"\"\n",
    "    if args.is_dynamic_loss_scale:\n",
    "        print(f\"=> Using DynamicLossScaleUpdateCell\")\n",
    "        scale_sense = nn.wrap.loss_scale.DynamicLossScaleUpdateCell(loss_scale_value=2 ** 24, scale_factor=2,\n",
    "                                                                    scale_window=2000)\n",
    "    else:\n",
    "        print(f\"=> Using FixedLossScaleUpdateCell, loss_scale_value:{args.loss_scale}\")\n",
    "        scale_sense = nn.wrap.FixedLossScaleUpdateCell(loss_scale_value=args.loss_scale)\n",
    "    net_with_loss = TrainClipGrad(net_with_loss, optimizer, scale_sense=scale_sense,\n",
    "                                  clip_global_norm_value=args.clip_global_norm_value,\n",
    "                                  use_global_norm=True)\n",
    "    return net_with_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tools/optimizer.py  \n",
    "from .schedulers import get_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\"multistep_lr\", \"cosine_lr\", \"constant_lr\", \"get_policy\", \"exp_lr\"]\n",
    "\n",
    "\n",
    "def get_policy(name):\n",
    "    \"\"\"get lr policy from name\"\"\"\n",
    "    if name is None:\n",
    "        return constant_lr\n",
    "\n",
    "    out_dict = {\n",
    "        \"constant_lr\": constant_lr,\n",
    "        \"cosine_lr\": cosine_lr,\n",
    "        \"multistep_lr\": multistep_lr,\n",
    "        \"exp_lr\": exp_lr,\n",
    "    }\n",
    "\n",
    "    return out_dict[name]\n",
    "\n",
    "\n",
    "def constant_lr(args, batch_num):\n",
    "    \"\"\"Get constant lr\"\"\"\n",
    "    learning_rate = []\n",
    "\n",
    "    def _lr_adjuster(epoch):\n",
    "        if epoch < args.warmup_length:\n",
    "            lr = _warmup_lr(args.warmup_lr, args.base_lr, args.warmup_length, epoch)\n",
    "        else:\n",
    "            lr = args.base_lr\n",
    "\n",
    "        return lr\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        for batch in range(batch_num):\n",
    "            learning_rate.append(_lr_adjuster(epoch + batch / batch_num))\n",
    "    learning_rate = np.clip(learning_rate, args.min_lr, max(learning_rate))\n",
    "    return learning_rate\n",
    "\n",
    "\n",
    "def exp_lr(args, batch_num):\n",
    "    \"\"\"Get exp lr \"\"\"\n",
    "    learning_rate = []\n",
    "\n",
    "    def _lr_adjuster(epoch):\n",
    "        if epoch < args.warmup_length:\n",
    "            lr = _warmup_lr(args.warmup_lr, args.base_lr, args.warmup_length, epoch)\n",
    "        else:\n",
    "            lr = args.base_lr * args.lr_gamma ** epoch\n",
    "\n",
    "        return lr\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        for batch in range(batch_num):\n",
    "            learning_rate.append(_lr_adjuster(epoch + batch / batch_num))\n",
    "    learning_rate = np.clip(learning_rate, args.min_lr, max(learning_rate))\n",
    "    return learning_rate\n",
    "\n",
    "\n",
    "def cosine_lr(args, batch_num):\n",
    "    \"\"\"Get cosine lr\"\"\"\n",
    "    learning_rate = []\n",
    "\n",
    "    def _lr_adjuster(epoch):\n",
    "        if epoch < args.warmup_length:\n",
    "            lr = _warmup_lr(args.warmup_lr, args.base_lr, args.warmup_length, epoch)\n",
    "        else:\n",
    "            e = epoch - args.warmup_length\n",
    "            es = args.epochs - args.warmup_length\n",
    "            lr = 0.5 * (1 + np.cos(np.pi * e / es)) * args.base_lr\n",
    "\n",
    "        return lr\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        for batch in range(batch_num):\n",
    "            learning_rate.append(_lr_adjuster(epoch + batch / batch_num))\n",
    "    learning_rate = np.clip(learning_rate, args.min_lr, max(learning_rate))\n",
    "    return learning_rate\n",
    "\n",
    "\n",
    "def multistep_lr(args, batch_num):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    learning_rate = []\n",
    "\n",
    "    def _lr_adjuster(epoch):\n",
    "        lr = args.base_lr * (args.lr_gamma ** (epoch / args.lr_adjust))\n",
    "        return lr\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        for batch in range(batch_num):\n",
    "            learning_rate.append(_lr_adjuster(epoch + batch / batch_num))\n",
    "    learning_rate = np.clip(learning_rate, args.min_lr, max(learning_rate))\n",
    "    return learning_rate\n",
    "\n",
    "\n",
    "def _warmup_lr(warmup_lr, base_lr, warmup_length, epoch):\n",
    "    \"\"\"Linear warmup\"\"\"\n",
    "    return epoch / warmup_length * (base_lr - warmup_lr) + warmup_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from src.tools.optimizer import get_optimizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate(args, batch_num):\n",
    "    \"\"\"Get learning rate\"\"\"\n",
    "    return get_policy(args.lr_scheduler)(args, batch_num)\n",
    "\n",
    "\n",
    "def get_optimizer(args, model, batch_num):\n",
    "    \"\"\"Get optimizer for training\"\"\"\n",
    "    print(f\"=> When using train_wrapper, using optimizer {args.optimizer}\")\n",
    "    args.start_epoch = int(args.start_epoch)\n",
    "    optim_type = args.optimizer.lower()\n",
    "    params = get_param_groups(model)\n",
    "    learning_rate = get_learning_rate(args, batch_num)\n",
    "    step = int(args.start_epoch * batch_num)\n",
    "    accumulation_step = int(args.accumulation_step)\n",
    "    learning_rate = learning_rate[step::accumulation_step]\n",
    "    train_step = len(learning_rate)\n",
    "    print(f\"=> Get LR from epoch: {args.start_epoch}\\n\"\n",
    "          f\"=> Start step: {step}\\n\"\n",
    "          f\"=> Total step: {train_step}\\n\"\n",
    "          f\"=> Accumulation step:{accumulation_step}\")\n",
    "    learning_rate = learning_rate * args.batch_size * int(os.getenv(\"DEVICE_NUM\", args.device_num)) / 512.\n",
    "    if accumulation_step > 1:\n",
    "        learning_rate = learning_rate * accumulation_step\n",
    "\n",
    "    if optim_type == \"momentum\":\n",
    "        optim = Momentum(\n",
    "            params=params,\n",
    "            learning_rate=learning_rate,\n",
    "            momentum=args.momentum,\n",
    "            weight_decay=args.weight_decay\n",
    "        )\n",
    "    elif optim_type == \"adamw\":\n",
    "        optim = AdamWeightDecay(\n",
    "            params=params,\n",
    "            learning_rate=learning_rate,\n",
    "            beta1=args.beta[0],\n",
    "            beta2=args.beta[1],\n",
    "            eps=args.eps,\n",
    "            weight_decay=args.weight_decay\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"optimizer {optim_type} is not supported\")\n",
    "\n",
    "    return optim\n",
    "\n",
    "\n",
    "def get_param_groups(network):\n",
    "    \"\"\" get param groups \"\"\"\n",
    "    decay_params = []\n",
    "    no_decay_params = []\n",
    "    for x in network.trainable_params():\n",
    "        parameter_name = x.name\n",
    "        if parameter_name.endswith(\".weight\"):\n",
    "            # Dense or Conv's weight using weight decay\n",
    "            decay_params.append(x)\n",
    "        else:\n",
    "            # all bias not using weight decay\n",
    "            # bn weight bias not using weight decay, be carefully for now x not include LN\n",
    "            no_decay_params.append(x)\n",
    "\n",
    "    return [{'params': no_decay_params, 'weight_decay': 0.0}, {'params': decay_params}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_swintransformer(args):\n",
    "    \"\"\"get swintransformer according to args\"\"\"\n",
    "    # override args\n",
    "    image_size = args.image_size\n",
    "    patch_size = args.patch_size\n",
    "    in_chans = args.in_channel\n",
    "    embed_dim = args.embed_dim\n",
    "    depths = args.depths\n",
    "    num_heads = args.num_heads\n",
    "    window_size = args.window_size\n",
    "    drop_path_rate = args.drop_path_rate\n",
    "    mlp_ratio = args.mlp_ratio\n",
    "    qkv_bias = True\n",
    "    qk_scale = None\n",
    "    ape = args.ape\n",
    "    patch_norm = args.patch_norm\n",
    "    print(25 * \"=\" + \"MODEL CONFIG\" + 25 * \"=\")\n",
    "    print(f\"==> IMAGE_SIZE:         {image_size}\")\n",
    "    print(f\"==> PATCH_SIZE:         {patch_size}\")\n",
    "    print(f\"==> NUM_CLASSES:        {args.num_classes}\")\n",
    "    print(f\"==> EMBED_DIM:          {embed_dim}\")\n",
    "    print(f\"==> NUM_HEADS:          {num_heads}\")\n",
    "    print(f\"==> DEPTHS:             {depths}\")\n",
    "    print(f\"==> WINDOW_SIZE:        {window_size}\")\n",
    "    print(f\"==> MLP_RATIO:          {mlp_ratio}\")\n",
    "    print(f\"==> QKV_BIAS:           {qkv_bias}\")\n",
    "    print(f\"==> QK_SCALE:           {qk_scale}\")\n",
    "    print(f\"==> DROP_PATH_RATE:     {drop_path_rate}\")\n",
    "    print(f\"==> APE:                {ape}\")\n",
    "    print(f\"==> PATCH_NORM:         {patch_norm}\")\n",
    "    print(25 * \"=\" + \"FINISHED\" + 25 * \"=\")\n",
    "    model = SwinTransformer(image_size=image_size,\n",
    "                            patch_size=patch_size,\n",
    "                            in_chans=in_chans,\n",
    "                            num_classes=args.num_classes,\n",
    "                            embed_dim=embed_dim,\n",
    "                            depths=depths,\n",
    "                            num_heads=num_heads,\n",
    "                            window_size=window_size,\n",
    "                            mlp_ratio=mlp_ratio,\n",
    "                            qkv_bias=qkv_bias,\n",
    "                            qk_scale=qk_scale,\n",
    "                            drop_rate=0.,\n",
    "                            drop_path_rate=drop_path_rate,\n",
    "                            ape=ape,\n",
    "                            patch_norm=patch_norm)\n",
    "    # print(model)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    assert args.crop, f\"{args.arch} is only for evaluation\"\n",
    "    set_seed(args.seed)\n",
    "    mode = {\n",
    "        0: context.GRAPH_MODE,\n",
    "        1: context.PYNATIVE_MODE\n",
    "    }\n",
    "    context.set_context(mode=mode[args.graph_mode], device_target=args.device_target)\n",
    "    context.set_context(enable_graph_kernel=True)\n",
    "    if args.device_target == \"Ascend\":\n",
    "        context.set_context(enable_auto_mixed_precision=True)\n",
    "    rank = set_device(args)\n",
    "\n",
    "    # get model and cast amp_level\n",
    "    net = get_swintransformer(args)\n",
    "    cast_amp(net)\n",
    "    criterion = get_criterion(args)\n",
    "    net_with_loss = NetWithLoss(net, criterion)\n",
    "    \n",
    "\n",
    "    data = get_dataset(args)\n",
    "    batch_num = data.train_dataset.get_dataset_size()\n",
    "    optimizer = get_optimizer(args, net, batch_num)\n",
    "    # save a yaml file to read to record parameters\n",
    "\n",
    "    net_with_loss = get_train_one_step(args, net_with_loss, optimizer)\n",
    "\n",
    "    eval_network = nn.WithEvalCell(net, criterion, args.amp_level in [\"O2\", \"O3\", \"auto\"])\n",
    "    eval_indexes = [0, 1, 2]\n",
    "    model = Model(net_with_loss, metrics={\"acc\", \"loss\"},\n",
    "                  eval_network=eval_network,\n",
    "                  eval_indexes=eval_indexes)\n",
    "\n",
    "    config_ck = CheckpointConfig(save_checkpoint_steps=data.train_dataset.get_dataset_size(),\n",
    "                                 keep_checkpoint_max=args.save_every)\n",
    "    time_cb = TimeMonitor(data_size=data.train_dataset.get_dataset_size())\n",
    "\n",
    "    ckpt_save_dir = \"./ckpt_\" + str(rank)\n",
    "\n",
    "    ckpoint_cb = ModelCheckpoint(prefix=args.arch + str(rank), directory=ckpt_save_dir,\n",
    "                                 config=config_ck)\n",
    "    loss_cb = LossMonitor()\n",
    "    eval_cb = EvaluateCallBack(model, eval_dataset=data.val_dataset, src_url=ckpt_save_dir,\n",
    "                               train_url=os.path.join(args.train_url, \"ckpt_\" + str(rank)),\n",
    "                               save_freq=args.save_every)\n",
    "\n",
    "    print(\"begin train\")\n",
    "    model.train(int(args.epochs - args.start_epoch), data.train_dataset,\n",
    "                callbacks=[time_cb, ckpoint_cb, loss_cb, eval_cb],\n",
    "                dataset_sink_mode=True)\n",
    "    print(\"train success\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation  \n",
    "eval.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mindspore')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b1b36678bc4607efff1f73e630bb667aa67b335ecbf744818fc0b0e10ae1e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
